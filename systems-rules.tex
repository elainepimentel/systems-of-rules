\documentclass{llncs}
\usepackage{xspace}
\usepackage{proof}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{leftidx}
\usepackage{cmll}
\usepackage{amssymb}
\usepackage{graphicx} % for rotatebox
\usepackage{rotating}
\usepackage{subcaption}
%\usepackage[hide]{ed}
\usepackage[show]{ed}
\usepackage[colorlinks,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage{xifthen}

\usepackage{collectbox}

\makeatletter
\newcommand{\mybox}{%
    \collectbox{%
        \setlength{\fboxsep}{1pt}%
        \fbox{\BOXCONTENT}%
    }%
}
\makeatother

\bibliographystyle{alpha}
\input{pheadf.tex}

\title{Systems of rules}

\author{Draft}
\institute{\today}
%\address{Dept. of Computer Science, University College London, UK}
%\date{\vskip-20pt\noindent Original version: November 2021}


\begin{document}
\maketitle

\begin{abstract}
In these notes, we will briefly present some ideas behind axioms and rules, and how they apply to mathematical explanations, trying to adapt the focusing ideas to sequents with contexts.
\end{abstract}

\section{What is focusing?}
Suppose that, in intuitionistic logic, we would like to try to prove the following sequent in intuitionistic logic
\[
A\iimp B, C\wedge D\seq p
\]
where $p$ is an atomic formula. There are two ways of proceeding with proof search (hence in a bottom-up reading of rules): either apply the conjunction left rule or the implication left rule.
%many ways of doing so. For example, we could start by applying a rule over the main connective of $F$, or one of the formulas in $\Gamma$, if they exist. Another possibility would be start with 

If we decide for the first, assuming (the multiplicative version of) the conjunction left rule, the derivation would look like
\[
\infer[\wedge_{l_M}]{A\iimp B, C\wedge D\seq p}
{\deduce{A\iimp B, C, D\seq p}{
\deduce{}{\vdots}}}
\] 
Since $(\wedge_{l_M})$ is an invertible rule~\cite{troelstra00book}, its application does not affect provability in the sense that, if the conclusion is provable, so is the premise.

Now, if instead we decide to apply the left rule for implication\footnote{We will avoid the discussion on copying the  implication to the left premise here. See~\cite{DBLP:journals/jsyml/Dyckhoff92}.}
\[
\infer[\iimp_{l}]{A\iimp B, C\wedge D\seq p}{C\wedge D\seq A & B, C\wedge D\seq p}
\] 
then provability may be lost, since $(\iimp_{l})$ is not invertible w.r.t. the left premise, that is, it may be the case that the conclusion sequent is provable but the left premise sequent is not.

Hence, when  searching for a proof of $A\iimp B, C\wedge D\seq p$ we can {\em always} start by applying the $(\wedge_{l_M})$ --  {\em don't care non-determinism} -- postponing making decisions, like applying $(\iimp_{l})$, until there are no more invertible options left. At that point, a decision has to be made: which non-invertible step should be taken --  {\em don't know non-determinism}. 

This is the essence of focusing~\cite{andreoli92jlc}: separate the proof steps into unfocused (no changes in provability) and focused (decisions must be made). In the focused step, once we decide to {\em focus on} (or work on) a formula, this focus is maintained, bottom-up, after the application of the rule. We mark the focused formulas with the downarrow symbol $\Downarrow$. The focused left rule for the implication is
\[
\infer[\iimp_{l}]{\jLf{\Gamma}{A\iimp B}{C}}{\jRf{\Gamma}{A}{} & \jLf{\Gamma}{B}{C}}
\] 
We read this as: if we decide to decompose $A\iimp B$ on the left, we should continue the proof search by maintaining the focus  on $A$ on the right and $B$ on the left.

But what about {\em atomic formulas}? Suppose that $A,B$ are atomic formulas in the focused derivation above. It is
        possible to impose two different ``protocols'' for dealing with the atomic case.  The $Q$-protocol insists that the left premise above is trivial, meaning that it is proved by
        the initial rule.  On the other hand, focusing should be lost (represented by the uparrow $\Uparrow$) in the right premise
\[
\infer[\iimp_{l}]{\jLf{\Gamma}{A\iimp B}{C}}
{\infer[\kinit]{\jRf{\Gamma}{A}{}}{} & 
\infer{\jLf{\Gamma}{B}{C}}{\jUnf{\Gamma}{B}{C}{}}}
\]  
        Following that protocol, we have that it should be the case that
        $A\in\Gamma$.  Thus, if we set $\Gamma'$
        to be the result of removing all occurrences of $A$
        from $\Gamma$, then the (unfocused) derived inference rule from the derivation above becomes
\[
  \infer[\iimp_{l_Q}]
        {\Gamma', A, A\iimp B \seq C}
        {\Gamma' , B\seq C}
\]
        The second protocol, the $T$-protocol, insists that the right-most
        premise is trivial and focus should be lost in the left premise
\[
\infer[\iimp_{l}]{\jLf{\Gamma}{A\iimp B}{C}}
{\infer{\jRf{\Gamma}{A}{}}{\jUnf{\Gamma}{}{A}{}} & 
\infer[\kinit]{\jLf{\Gamma}{B}{C}}{}}
\]   
That is,
        $B$ and $C$ are the same atomic formula.       
Thus,  the (unfocused) derived inference rule from the derivation above becomes
\[
  \infer[\iimp_{l_T}]
        {\Gamma, A\iimp B \seq B}
        {\Gamma \seq A}
\]
The names for the $Q$ and $T$ protocols comes from Danos, Joinet, and
Schellinx~\cite{danos93wll}: in the $Q$ protocol, the tail (``queue'')
of an implication yields a trivial premise while in the $T$ protocol,
the head (``t\^ete'') of an implication yields a trivial premise.

A more modern and flexible presentation of the $Q$ and $T$ protocols
speaks, instead, of the \emph{polarity} of formulas.
%
In particular, if all atomic formulas have a {\em positive polarity}, the
$Q$-protocol is enforced, while if all atomic formulas have a {\em negative
polarity}, the $T$-protocol is enforced.

Now that focusing is understood for implication and atoms, what can be said about the other connectives? For example, in the case of conjunction, we adopted the (invertible) {\em multiplicative version} of the left rule, that is
\[
\infer[\wedge_{l_M}]{\Gamma, A\wedge B\seq C}{\Gamma, A, B\seq C}
\] 
This rule incorporates the left contraction rule. Gentzen's original rules correspond to the (non-invertible) {\em additive version}, where a choice has to be made during proof search
\[
 \infer[\wedge_{l_{i}}]{A_1\wedge A_2, \Gamma \seq C}{A_i, \Gamma \seq C}
\]
Such multiplicative/additive, invertible/non-invertible flavors can be captured in a single proof system by splitting the conjunction into two connectives: 
$\wedgep$ and $\wedgen$, with {\em unfocused/focused} left rules 
%Of course we have $A\wedgen B$ is logically equivalent to $A\wedgep B$, so while provability is maintained, the form of the resulting proofs can be completely different.
\[
  \infer[\wedgep_{l}]{\jUnf{\Gamma}{A\wedgep B,\Theta}{}{C}}{\jUnf{\Gamma}{A , B, \Theta}{}{C}}
  \qquad
 \infer[\wedgen_{l_i}]{\jLf{\Gamma}{A_1 \wedgen A_2}{C}}{\jLf{\Gamma}{A_i}{C}}
 \]


In \cite{liang07csl,LiaMil09} Miller and Liang proposed the  \LKF and \LJF focused proof
systems for classical and intuitionistic logics, respectively.
%
Those systems 
extend both the notion of focusing and polarity to
all formulas. 

In such systems, {\em focused rule applications} imply that focus is transferred from conclusion to premises in derivations. This process goes on until either the focused phase ends (depending on the {\em polarity} of the focused formula), or the derivation ends.
Once the focus is \emph{released}, the formula is eagerly decomposed into subformulas, which are ultimately {\em stored} in the context. 
%
We will describe this in detail in the next section.

{\em Some historical remarks.} The focusing discipline is based on the notion of {\em uniform proofs}~\cite{miller91apal} and it was discovered by Andreoli in~\cite{andreoli92jlc}, who showed that it is complete for linear logic~\cite{DBLP:journals/tcs/Girard87}, which naturally contains the multiplicative and additive versions of disjunction and conjunction. In intuitionistic logic focusing gives rise to  call-by-value~\cite{dyckhoff06cie}  and call-by-name~\cite{herbelin94csl} calculi, since using the $Q$-protocol the proof-search semantics of the
implication is given by 
\emph{forward-chaining}, while using the $T$-protocol, the proof-search semantics of the
 implication is given by
\emph{back-chaining}.

\section{Polarization}
\LKF and \LJF ~\cite{LiaMil09}can accommodate both the $Q$ and $T$ protocols as well
as a mix of those protocols.
%
The proof system \LKF, for first-order classical logic, and the
proof system \LJF, for first-order intuitionistic logic, are
presented in Figures~\ref{fig:lkf} and~\ref{fig:ljf}, respectively.

In order to obtain their flexibility in capturing various focusing
regimes, the \LKF and \LJF proof systems use \emph{polarized}
formulas.
%
A \emph{polarized classical (first-order) formula} is a formula built
using atomic formulas, 
the usual first-order quantifiers $\forall$ and $\exists$, the implication
$\impl$, and polarized versions of the logical connectives and
constants, i.e., $\ntrue$, $\ptrue$,$\nfalse$, $\pfalse$, $\veen$,
$\veep$, $\wedgen$, $\wedgep$.
%
A \emph{polarized intuitionistic (first-order) formula} is a polarized
classical formula in which the logical connectives $\nfalse$ and
$\veen$ do not occur.
%
The positive and negative versions of connectives and constants
have identical truth conditions but different inference rules
inside the polarized proof systems.
%
For example, the left introduction rule for $\wedgep$ is invertible
while the left introduction rule for $\wedgen$ is not invertible.

%We shall also find it necessary to use \emph{delays}: if $B$ is a
%polarized formula then we define $\negd{B}$ to be (the always
%negative) $B\wedgen\ntrue$ and $\posd{B}$ to be (the always positive)
%$B\wedgep\ptrue$.
%%
%Equivalently, we can take $\posd{\cdot}$ to be the 1-ary version of
%either the binary $\veep$ or $\wedgep$ and
%take $\negd{\cdot}$ to be the 1-ary version of either
%the binary $\veen$ or $\wedgen$.
%%
%(The 0-ary version of these four connectives correspond to the
%logical units $\pfalse$, $\ptrue$, $\nfalse$, 
%%or 
%$\ntrue$.)

If a formula's top-level connective is $\ptrue$,
$\pfalse$, $\veep$, $\wedgep$, or $\exists$, then that formula is
\emph{positive}.
%
If a formula's top-level connective is $\ntrue$, $\nfalse$,
$\veen$, $\wedgen$, $\impl$, or $\forall$, then it is \emph{negative}.
%
Note that in the intuitionistic system \LJF, we have only one
disjunction and one falsum, both of which exist only with positive
polarity.
%
The way to form the negation of the polarized formula $B$ is with the
formula $B\impl\pfalse$: this formula has negative polarity no matter
the polarity of $B$.

%\ednote{MV: We said above that $\nfalse$ does not occur in the
%intuitionistic language. EP. I think that it should be $\pfalse$.} 

In both \LKF and \LJF, every polarized formula is classified as
positive or negative.
%
This means that we must also provide a polarity to atomic formulas.
%
As it turns out, this assignment of polarity to atomic formulas can,
in principle, be arbitrary.
%
%In particular, an \emph{atomic bias assignment} is a function
%$\bias{\cdot}$ that maps atomic formulas to the set of two tokens
%$\{pos,~neg\}$: if $\bias{A}$ is $pos$ then that atomic formula is
%positive and if $\bias{A}$ is $neg$ then that atomic formula is
%negative.
%%
%We may ask that all atomic formulas are positive, that
%they are all negative, or we can mix polarity
%assignments. 
%%
%In particular, the atomic bias assignment $\biasp{\cdot}$ assigns all
%atoms a positive polarity while $\biasn{\cdot}$ assigns all
%atoms a negative polarity.
%%
%We shall assume that an atomic bias
%assignment is also \emph{stable under substitution}: that is, for all
%substitutions $\theta$,  $\bias{\theta A} = \bias{A}$.
%%
%In first-order logic, this is equivalent to saying that such bias
%assignments are \emph{predicate determined}: that is, if atoms $A$ and
%$A'$ have the same predicate head, then $\bias{A}=\bias{A'}$.

%We say that the pair $\tup{\delta,\hat B}$ is a \emph{polarization of}
%$B$ if $\bias{\cdot}$ is an atomic bias assignment and if every
%occurrence of $\true$, $\wedge$, $\false$, and $\vee$ in $B$ is
%labeled with either the $+$ or $-$ annotation.
%%
%If $B$ has $n$ occurrences of these logical connectives then there are
%$2^n$ different ways to place these $+$ or $-$ symbols.
%%
%We shall also allow the insertion of any number of $\posd{\cdot}$ and
%$\negd{\cdot}$ into $\hat B$ as well.
%%
%In other words, the polarized formula $\tup{\delta,C}$ is a
%polarization of $B$ if deleting all delays and all $+$ and $-$
%annotations on logical connectives of $C$ results in $B$.
%
Note that we use $\impl$, $\forall$, and $\exists$ in both unpolarized
as well as polarized formulas: we can do this since the polarity of
these connectives is not ambiguous.
%
In classical logic, the polarity of $\true$, $\wedge$, $\false$, and
$\vee$ is ambiguous and all of these can be positive or negative.
%
In intuitionistic logic, only the polarity of $\true$ and $\wedge$
is ambiguous.
%
In both of these logics, however, the polarity of atoms is equally
ambiguous.
%
%Finally, if $\tup{\delta,\hat B}$ is a polarization of $B$, we shall
%generally drop explicit reference to $\delta$ and simply say that
%$\hat B$ is a polarization of $B$: often, the atomic bias assignment
%is either not important or can be inferred from context.


\subsection{Focused proof systems}
\label{sec:focused ps}


\begin{figure}
	{\sc Asynchronous Rules}
\[
  \infer[\kern-2pt\veen]{\jUnf{A \veen\kern-3pt B, \Delta}{\Gamma}}
                 {\jUnf{A,B, \Delta}{\Gamma}}   
  \qquad 
  \infer[\kern-2pt\wedgen]{\jUnf{A \wedgen B, \Delta}{\Gamma}}
                   {\jUnf{A, \Delta}{\Gamma}
                    & 
                    \jUnf{B, \Delta}{\Gamma}}
\]
\[
 \infer[\forall]{\jUnf{\forall x.B, \Delta}{\Gamma}}
                   {\jUnf{[y/x]B, \Delta}{\Gamma}}	
\qquad
 \infer[\kern-2pt\nfalse]{\jUnf{\nfalse\kern-2pt,\Delta}{\Gamma}}
                           {\jUnf{\Delta}{\Gamma}}
\qquad
\infer[\ntrue]{\jUnf{\ntrue\kern-3pt,\Delta}{\Gamma}}{}
\]
	
\medskip{\sc Synchronous Rules}	

%\[ 
%  \infer[\impl_l]{\jLf{\Gamma}{A\impl B}{\Delta}}
%                 {\jRf{\Gamma}{A}{\Delta} &  \jLf{\Gamma}{B}{\Delta}}
%  \quad	
%  \infer[\veen_l]{\jLf{\Gamma}{A \veen\kern-3pt B}{\Delta}}
%                 {\jLf{\Gamma}{A}{\Delta}
%                  & 
%                  \jLf{\Gamma}{B}{\Delta}}
%  \quad
%  \infer[\kern -2pt\wedgen_l]{\jLf{\Gamma}{A_1 \wedgen\kern-3pt A_2}{\Delta}}
%                   {\jLf{\Gamma}{A_i}{\Delta}}
%\]
\[
  \infer[\wedgep]{\jRf{A \wedgep B}{\Gamma}}
                   {\jRf{A}{\Gamma}
                    & 
                    \jRf{B}{\Gamma}}
  \qquad
  \infer[\veep]{\jRf{A_1 \veep A_2}{\Gamma}}
                 {\jRf{A_i}{\Gamma}}
\]
\[
  \infer[\exists]{\jRf{\exists x.B}{\Gamma}}
                   {\jRf{[t/x]B}{\Gamma}}
  \qquad
  \infer[\ptrue]{\jRf{\ptrue}{\Gamma}}{}
  \]

\medskip{\sc Identity rule}
	
\[
  \infer[\kinit_r]{\jRf{\Gamma,P_a}{P_a}{\Delta}}{}
\]
	
\medskip{\sc Structural rules}
	
\[
  \infer[\kdecide]{\jUnf{}{P, \Gamma}}
                    {\jRf{P}{P, \Gamma}}
  \quad
  \infer[\krelease]{\jRf{N}{\Gamma}}
                     {\jUnf{N}{\Gamma}}
  \qquad
  \infer[\kstore]{\jUnf{D, \Delta}{\Gamma}}
                   {\jUnf{\Delta}{D, \Gamma}}
\]
	
Here, $P$ is positive, $N$ is negative, $C$ is a negative formula or
positive atom, $D$ a positive formula or negative atom, $N_a$ is
a negative atom, and $P_a$ is a positive atom.  Other formulas
are arbitrary. In the rule $\forall$  the
eigenvariable $y$ does not occur free in any formula of the
conclusion.
\caption{The focused classical sequent calculus \LKF.}
\label{fig:lkf}
\end{figure}	

\begin{figure}
{\sc Asynchronous Rules}

\[
  \infer[\impl_r]{\jUnfG{\Sigma}{A \supset B}}{\jUnfG{A,\Sigma}{B}}   
  \qquad
  \infer[\wedgen_r]{\jUnfG{\Sigma}{A \wedgen B}}
                   {\jUnfG{\Sigma}{A} \quad \jUnfG{\Sigma}{B}}
\]
\[
  \infer[\wedgep_l]{\jUnfGamb{A\wedgep B,\Sigma}}{\jUnfGamb{A , B, \Sigma}}
  \qquad
  \infer[\veep_l]{\jUnfGamb{A\veep B,\Sigma}}
        {\jUnfGamb{A,\Sigma}\quad \jUnfGamb{B,\Sigma}}
\]
\[
  \infer[\forall_r]{\jUnfG{\Sigma}{\forall x.B}}{\jUnfG{\Sigma}{[y/x]B}}
  \qquad
  \infer[\exists_l]{\jUnfGamb{\exists x.B, \Sigma}}{\jUnfGamb{[y/x]B,\Sigma}}
 \]
\[
  \infer[\ntrue_r]{\jUnfG{\Sigma}{\ntrue}}{}\qquad
  \infer[\ptrue_l]{\jUnfGamb{\ptrue, \Sigma}}{\jUnfGamb{\Sigma}}
   \qquad
  \infer[\pfalse_l]{\jUnfGamb{\pfalse, \Sigma}}{}
\]

\medskip{\sc Synchronous Rules}

\[ 
  \infer[\impl_l]{\jLfG{A \impl B}}{  \jRfG{A}\quad \jLfG{B}}
  \qquad
  \infer[\veep_r]{ \jRfG{A_1 \veep A_2}}{\jRfG{A_i}}
  \qquad
  \infer[\wedgen_l]{\jLfG {A_1 \wedgen A_2}}{\jLfG{A_i}}
\]
\[
  \infer[\wedgep_r]{\jRfG{A \wedgep B}}{\jRfG{A} \quad \jRfG{B}}
  \quad
  \infer[\forall_l]{\jLfG{\forall x.B}}{\jLfG{[t/x]B}}
  \quad
  \infer[\exists_r]{\jRfG{\exists x.B}}{\jRfG{[t/x]B}}
  \quad
  \infer[\ptrue_r]{\jRfG{\ptrue}}{}
\]

\medskip{\sc Identity rules}

\[
  \infer[\kinit_l]{\jLf{\Gamma}{N_a}{N_a}}{}
  \qquad
  \infer[\kinit_r]{\jRf{\Gamma,P_a}{P_a}}{}
  \qquad
%  \infer[cut]{\jUnf{\Gamma}{}{}{R}}
%             {\jUnf{\Gamma}{}{F}{}\qquad \jUnf{\Gamma}{F}{}{R}}
\]

\medskip{\sc Structural rules}

\[
  \infer[\kern -2pt \kdecide_l]{\jUnf{\Gamma,N}{}{}{R}}{\jLf{\Gamma,N}{N}{R}}
  \quad
  \infer[\kern -2 pt \kdecide_r]{\jUnf{\Gamma}{}{}{P}}{\jRf{\Gamma}{P}}
  \quad
  \infer[\krelease_l]{\jLf{\Gamma}{P}{R}}{\jUnf{\Gamma}{P}{}{R}}
  \quad
  \infer[\krelease_r]{\jRf{\Gamma}{N}}{ \jUnf{\Gamma}{}{N}{}}
\]
\[
\infer[\kstore_l]{\jUnfamb{\Gamma}{C,\Sigma}{}}
                 {\jUnfamb{C,\Gamma}{\Sigma}{}}
  \qquad
  \infer[\kstore_r]{\jUnf{\Gamma}{}{D}{}}{\jUnf{\Gamma}{}{}{D}}
\]

Here, $P$ is positive, $N$ is negative, $C$ is a negative formula or
positive atom, $D$ a positive formula or negative atom, $N_a$ is a
negative atom, and $P_a$ is a positive atom.  Other formulas are
arbitrary. $\Rscr$ denotes $\Delta_1 \Uparrow \Delta_2$ where the
union of $\Delta_1$ and $\Delta_2$ contains at most one formula.  
In the rules $\forall_r$  and $\exists_l$ the
eigenvariable $y$ does not occur free in any formula of the
conclusion.
\caption{The focused intuitionistic sequent calculus \LJF.}
\label{fig:ljf}
\end{figure}	

The inference rules of \LKF and \LJF presented in Figures~\ref{fig:lkf} and~\ref{fig:ljf}, respectively, involve three kinds of sequents 
\begin{itemize}
\item $\jUnf{\Gamma}{\Sigma}{\Omega}{\Delta}$ belongs to the {\em asynchronous phase}. During this phase, all negative (resp. positive) formulas in  $\Omega$ (resp. $\Sigma$) are introduced and all atoms and positive (resp. negative) formulas  are stored in the context $\Delta$ (resp. $\Gamma$);
\item $\jLf{\Gamma}{B}{\Delta},\quad\hbox{and}\quad
  \jRf{\Gamma}{B}{\Delta}$ belongs to the {\em synchronous phase}, where all positive connectives at the root of $B$ are introduced. 
\end{itemize}
Here $\Gamma$, $\Sigma$, $\Omega$ and $\Delta$ are
multisets of polarized formulas and $B$ is a polarized formula.
%
The formula occurrence $B$ in a $\Downarrow$-sequent is called the
\emph{focus} of that sequent.

\cyan{The dynamics of the different phases is detailed below.}

\noindent
\cyan{{\bf Asynchronous Phase.} Asunchronous rules can be applied eagerly, in any order.
This process includes storing formulas: note that the rules $\kstore_r, \kstore_l$ move, to the 
contexts $\Gamma/\Delta$, atomic or negative/positive formulas since they cannot be decomposed during the asynchronous phase.}

\cyan{The asynchronous phase ends when $\Sigma,\Omega$ in unfocused sequents are empty. A decision rule $\mathsf{D_l}$ or  $\mathsf{D_r}$ is then applied,  and
a synchronous phase starts by {\em focusing} on a non-atomic formula, either negative on the left or positive on the right.}

\cyan{
\noindent
{\bf Synchronous Phase.} Once we focus on a formula, the proof  follows by applying synchronous rules, where  the focus persists on the decomposed subformulas until either: the proof ends with an instance of an axiom;
a negative (resp. positive) formula  on the right (resp. left) is reached, and the synchronous phase  ends with the application of one of the release rules $\krelease_r,\krelease_l$.
}

\cyan{
There are two initial rules, $\kinit_r,\kinit_l$, acting on right positive atoms and left negative atoms, respectively, meaning that derivations should necessarily end when focusing on such formulas.}


The system  \LJF is depicted in a separate figure for the sake of clarity.
%
However, one can notice that, similarly to what we have for \LJ and
\LK in the original Gentzen formulations, \LJF can be seen as a
restriction of \LKF, where the rules for $\nfalse$ and $\veen$ are
omitted and only one formula is admitted in the succedent of sequents.  In
particular, this implies that $(i)$ in the left rule for $\impl$, the
right context of the conclusion is not present in the left premise;
$(ii)$ in the rule $\kdecide_r$, the formula placed under focus is
not subjected to contraction; and $(iii)$ a sequent of the form $\jRf{\Gamma}{B}{\Delta}$, when used in an
\LJF proof, is such that $\Delta$ is empty.
%
In that case, we write that sequent as simply $\jRf{\Gamma}{B}{}$.



\section{Axioms to rules: bipoles}
In~\cite{DBLP:journals/apal/MarinMPV22} we have presented a process of transforming (polarized) bipolar axioms into rules in the
classical/intuitionistic settings. 
The idea is that bipolars force a unique {\em shape} in focused derivations (called {\em bipoles}), where only atoms are stored in the leaves.
Bipoles then {\em justifies} a synthetic inference rule for the respective bipolar.\footnote{While it should be noted that a bipolar can give rise to different bipoles, they do not differ in their {\em shape}.}

We will illustrate the process with an example.
\begin{example}\label{ex:bipole}
Let $p(x)$, $q(x)$, and $r(x)$ be {\em negative} atomic
formulas and assume that the polarized formula $\forall x. (p(x)
 \wedgep q(x))\impl r(x)$ is a member of
$\Gamma$.
%
Consider the following \LKF derivation 
\[
  \infer[\kdecide_l]{\jUnf{\Gamma}{}{}{\Delta}}{
  \infer[\forall_l]{\jLf{\Gamma}{\forall x. (p(x)
                    \wedgep q(x))\impl r(x)}{\Delta}}
 {\infer[\impl_l]{\jLf{\Gamma}{(p(t)
                    \wedgep q(t))\impl r(t)}{\Delta}}
   {\infer[\wedgep_r]{\jRf{\Gamma}{p(t)\wedgep q(t)}{\Delta}}
   {\infer[\krelease_r]{\jRf{\Gamma}{p(t)}{\Delta}}
    {\infer[\kstore_r]{\jUnf{\Gamma}{}{p(t)}{\Delta}}
   {\deduce{\jUnf{\Gamma}{}{}{\Delta,p(t)}}{}}}
   &
   \infer[\krelease_r]{\jRf{\Gamma}{q(t)}{\Delta}}
   {\infer[\kstore_r]{\jUnf{\Gamma}{}{q(t)}{\Delta}}
   {\deduce{\jUnf{\Gamma}{}{}{\Delta,q(t)}}{}}}}
   & 
   \infer[\kinit_l]{\jLf{\Gamma}{ r(t)}{\Delta}}
  {}}}}
\]	
%
In order to apply the rule $\kinit_l$ in this derivation, it must be
the case that $r(t) \in\Delta$, that is, $r(t)$ is in the conclusion sequent. The atomic predicates $p(t), q(t)$ appear stored in the leaves, so they are in the premises. 
%
This derivation justifies the following (unfocused) synthetic inference rule in \LK,
on unpolarized formulas % computed by this synthetic rule 
\[
  \infer{\Gamma\seq \Delta, r(t)}
        {\Gamma\seq \Delta, p(t) \qquad 
         \Gamma\seq \Delta, q(t)}	
\]
%Let $P_1(x)$, $P_2(x)$, $Q(x)$, and $R(x,y)$ be positive atomic
%formulas and assume that the polarized formula $\forall x (((P_1(x)
%\impl P_2(x)) \wedgep Q(x))\impl\exists y R(x,y))$ is a member of
%$\Gamma$.
%%
%Consider the following \LJF derivation 
%\[
%  \infer[\kdecide_l]{\jUnf{\Gamma}{}{}{C}}{
%  \infer[\forall_l]{\jLf{\Gamma}{\forall x(((P_1(x)\impl P_2(x))
%                    \wedgep Q(x))\impl\exists y R(x,y))}{C}}
% {\infer[\impl_l]{\jLf{\Gamma}{((P_1(t) \impl P_2(t)) \wedgep Q(t))
%                       \impl \exists y R(t,y)}{C}}
% {\infer[\wedgep_r]{\jRf{\Gamma}{(P_1(t)\impl P_2(t))\wedgep Q(t)}{}}
% {\infer[\krelease_r]{\jRf{\Gamma}{P_1(t)\impl P_2(t)}{}}
%   {\infer[\impl_r]{\jUnf{\Gamma}{}{P_1(t)\impl P_2(t)}{}}
%   {\infer[\kstore_l]{\jUnf{\Gamma}{P_1(t)}{P_2(t)}{}}
%   {\infer[\kstore_r]{\jUnf{\Gamma,P_1(t)}{}{P_2(t)}{}}
%   {\deduce{\jUnf{\Gamma,P_1(t)}{}{}{P_2(t)}}{}}}}}
%   &
%   \infer[\kinit_r]{\jRf{\Gamma}{Q(t)}{}}{}}
%   & 
%   \infer[\krelease_l]{\jLf{\Gamma}{\exists y R(t,y)}{C}}
%  {\infer[\exists_l]{\jUnf{\Gamma}{\exists y R(t,y)}{}{C}}
%  {\infer[\kstore_l]{\jUnf{\Gamma}{R(t,z)}{}{C}}
%  {\deduce{\jUnf{\Gamma, R(t,z)}{}{}{C}}{}}}}}}}
%\]	
%%
%In order to apply the rule $\kinit_r$ in this derivation, it must be
%the case that $Q(t)\in\Gamma$, that is, $Q(t)$ is in the conclusion. The atomic predicates $P_1(t), P_2(t), R(t,z)$ appear stored in the leaves, so they are in the premises. 
%%
%This derivation justifies the following synthetic inference rule in \LJ,
%on unpolarized formulas % computed by this synthetic rule 
%\[
%  \infer{Q(t),\Gamma\vdash C}
%        {P_1(t), Q(t), \Gamma\vdash P_2(t) \qquad 
%         R(t,z), Q(t), \Gamma\vdash C}	
%\]
%where the variable $z$ does not occur in the conclusion. 
\end{example}
\cyan{In this note, we intend to define the idea of axioms-as-rules in a context, so that rules can be applied deep inside (special kinds of) formulas.}

Although this has a very close inspiration in {\em deep inference}~\cite{guglielmi07tocl} and rule discharging~\cite{DBLP:journals/sLogica/Schroeder-Heister14}, we will maintain the tree structure of proofs, in a sequent style setting.

\section{Mathematical explanations}
When proving a theorem, often mathematicians make use of defined objects, a mathematical theory and some lemmas, which have been proven before. 
The argumentation is done via logical reasoning, which organizes the proof into a tree-structure, where such objects, theories and theorems are used in specific places.  

Internal mathematical explanations are proofs of mathematical theorems that seem to {\em explain} the theorem they prove. A rigorous analysis of these explanations were studied in~\cite{francesca} and~\cite{Poggiolesi2023}. The main idea is  to move from informal proofs of mathematical theorems to a formal presentation that involves proof-trees together with a decomposition of their elements into assumptions, conclusion and rules. Then show that those mathematical proofs that are regarded as having an explanatory power all have the same formal presentation: they display an increase of conceptual complexity from the assumptions to the conclusion.

As Poggiolesi affirms in~\cite{francesca}:
\begin{quote}
``we will look at formal proofs as trees in which the assumptions or premises of the mathematical proof are the leaves of the tree, the conclusion of the mathematical proof is the root of the tree, and each rule applied in the proof links the nodes corresponding to the premises of the rule to the node corresponding to the conclusion of the rule.''
\end{quote}

There are, however, many ways of formalizing an informal proof, giving rise to possibly several different proof-trees, depending on the role assigned in the proof-tree to the components of the informal proof.

Francesca~\cite{francesca} suggests the following proof organization: 
\begin{enumerate}[a.]
\item the {\em grounds} (or reasons why a theorem is true) are captured by undischarged assumptions  that can be shown to be conceptually less complex than the theorem itself;
\item  the {\em generalization}, that links cause and effect, are primarily given by axioms ruling the laws. 
\end{enumerate}
While (a) implies that derivations should necessarily be {\em open}, where the leaves are grounds, (b) suggests that the axioms-as-rules approach can help on pushing this research plan forward by systematically transforming such axioms into inference rules.

\subsection{From informal mathematical proofs to proof-trees -- a case study}
As a case study, we will analise an interesting example presented in~\cite{francesca}, the {\em Circle Theorem}:

\begin{theorem} Given two circles $A$ and $B$, one with center $a$ and radius $ab$, and the other with center $b$ and radius $ab$, then there always exists a point $c$ where they intersect such that $l(ac) = l(cb) = l(ab)$.
\end{theorem}
In the proof, the discharged hypothesis is $Circ(A, a, ab) \wedge Circ(B, b, ab)$ and the ground is 
%\[
\begin{equation}\label{ground1}
Point(a) \to (Point(b) \to\exists c. Point(c) \wedge l(ab) = l(ac) = l(bc))
\end{equation}%\]
The generalizations, on the other hand, are stated as the (natural deduction) rules
\[
\infer[c-p]{Point(b)}{Circ(B,b,ab)} \qquad
\infer[c-p+]{\exists c. Point(c) \wedge c\in A\wedge c\in B\wedge l(ab) = l(ac) = l(bc)}{\exists c. Point(c) \wedge l(ab) = l(ac) = l(bc)}
\]
There are two interesting facts about this proof. The first is that ground~\ref{ground1} is less complex than the conclusion of the theorem
\[
Circ(A, a, ab) \wedge Circ(B, b, ab)\to\exists c.Point(c)\wedge c\in A\wedge c\in B\wedge l(ab)=l(ac)=l(bc)
\]
I find the discussion about the asymmetry of inverting the proof (which is often done in basic Euclidean Geometry textbooks) particularly fascinating, since proof explanations depend on starting from less complex grounds to more complex results.

The second is that, although the rule $c-p$ (in a sequent formulation) corresponds to the axiom 
\[\forall B,a,b.Circ(B,b,ab)\to Point(b)\]
with both atomic predicates {\em negative}, the rule $c-p+$ {\em does not} correspond to 
\[
\exists c. Point(c) \wedge l(ab) = l(ac) = l(bc) \to \exists c. Point(c) \wedge c\in A\wedge c\in B\wedge l(ab) = l(ac) = l(bc)
\]
In fact, it corresponds to the axiom 
\[
f \to  (c\in A\wedge c\in B)
\] 
deeply applied in the context $\mathcal{C} = \exists c. Point(c) \wedge l(ab) = l(ac) = l(bc)\wedge\{\cdot\}$, that is
\[
\infer{\mathcal{C}\{c\in A\wedge c\in B\}}{\mathcal{C}\{\cdot\}}
\]
That is, $\exists c.Point(c)\wedge l(ab) = l(ac) = l(bc)$ would serve as the {\em context} and rules would be applied deeply on {\em holes}.


\section{A sequent system approach} 
It is somehow folklore that natural deduction proofs correspond to negative polarised sequent proofs (see~\cite{DBLP:journals/entcs/PimentelNN16}), since we intend to directly use the method developed in~\cite{DBLP:journals/apal/MarinMPV22} to mathematical proof explanation, we will move to sequent systems.

\cyan{
\begin{goal}
Leave the folklore and establish this formally.
\end{goal}}


In the unpublished draft~\cite{francesca}, Poggiolesi aims at providing a rigorous analysis of mathematical explanations. In what follows, we will try to formalize this reasoning using a deep notion of focusing. 

The discussion will be restricted to the classical case, and formulas are assumed to be in negation normal form. We call {\em literal} an atomic formula $p$ or its negation $\ov{p}$. If $p$ is a negative/positive literal, $\ov{p}$ will be its positive/negative dual.\footnote{\magenta{Better to add negation as an operation, as Girard does in LC.}}

If $A$ is a formula, then $\atm{A}$ will denote the {\em freezing} of $A$, meaning that $A$ will be considered a (negative) atom. We will denote by $\atmp{A}$ the positive dual of $\atm{A}$, that is, $\atmp{A}=\ov{\atm{A}}$.\footnote{\magenta{Better to define a function $\atmp{\cdot}$.}}

If $A$ is either a literal or a frozen formula, we will call it {\em terminal}.
\begin{definition}\label{def:seq}
Let $\Gamma$ be a multiset of (first-order classical) polarized formulas and $\mathcal{C}, B$ be polarized formulas. 

A {\em context} $\Cx{}$ is a formula with a \emph{hole} $\Ex{}$ where a formula would otherwise occur. 

{\em Focused context} sequents have the form
\[
  \jRCf{B}{\Gamma}
\]
The formula occurrence $B$ in a $\Downarrow$-sequent is called the
\emph{focus} of that sequent.
\end{definition}

The focused sequent system \LCF\  consists of the asynchronous rules for \magenta{one sided} \LKF\  plus the focused context rules in Figure~\ref{Fig:system}.

The intuition is the following: 
\begin{itemize}
\item after doing some (bottom-up) reasoning, we may want to focus on a formula deep inside a context;
\item this formula corresponds to a (bipole) axiom which should be translated into a (bipolar) rule;
\item we may want to ``finish'' before arriving to atoms. This will be handled by the freezing.
\end{itemize}
This means that:
\begin{itemize} 
\item focusing on terminal formulas on the right should finish the proof, forcing their duals to be present from the beginning; 
\item when focusing is lost, formulas become ``available" as premises. 
\end{itemize}
In a nutshell: 
\begin{itemize}
\item we focus ``deep inside'' formulas;
\item we lose focus when reaching suitable ``terminal formulas''.
\end{itemize}
This is a generalization of the focusing discipline since:
\begin{itemize}
\item if the context $\mathcal{C}$ is empty, we have the usual focused rules in Figure~\ref{fig:lkf};
\item if the terminal formulas are atoms, we have the usual polarized atomic treatment as in \LKF.
\end{itemize}



\begin{figure}[t]
%{\sc First-order rules}
%\[\begin{array}{lcl}
%\infer[{\wedge L}]{\Theta;A \wedge B,\Gamma \seq 
%\Delta}{
%\Theta;A, B,\Gamma \seq 
%\Delta} 
%& \qquad &
%\infer[{\wedge R}]{\Theta;\Gamma \seq 
%\Delta, A \wedge B}{\Theta;\Gamma \seq 
%\Delta, A &\Theta;\Gamma \seq 
%\Delta, B } 
%\\ \\
%\infer[{\iimp L}]{\Theta;A \iimp B,\Gamma \seq 
%\Delta}
%{\Theta;\Gamma \seq 
%\Delta, A &  \Theta;B,\Gamma \seq 
%\Delta} 
%& &
%\infer[{\iimp R}]{\Theta;\Gamma \seq 
%\Delta,A \iimp B}{\Theta;\Gamma, A \seq 
%\Delta,B}
%\\ \\
%\infer[{\vee L}]{\Theta; A \vee B,\Gamma \seq 
%\Delta}{\Theta;A,\Gamma \seq 
%\Delta &\Theta;B,\Gamma \seq 
%\Delta } 
%& \qquad &
%\infer[{\vee R}]{\Theta;\Gamma \seq 
%\Delta, A \vee B}{
%\Theta;\Gamma \seq 
%\Delta, A, B} 
%\\ \\
%\infer[{\exists L}]{\Theta;\exists x.A(x),\Gamma \seq 
%\Delta}{
%\Theta; A[y/x],\Gamma \seq 
%\Delta}
%& \qquad &
%\infer[{\exists R}]{\Theta;\Gamma \seq 
%\Delta, \exists x. A(x)}{
%\Theta;\Gamma \seq 
%\Delta, \exists x.A(x), A[t/x]}
%\\ \\
%\infer[{\forall L}]{\Theta;\forall x.A(x),\Gamma \seq 
%\Delta}{
%\Theta; \forall x. A(x),A[t/x],\Gamma \seq 
%\Delta}
%& \qquad &
%\infer[{\forall R}]{\Theta;\Gamma \seq 
%\Delta, \forall x. A(x)}{
%\Theta;\Gamma \seq 
%\Delta, A[y/x]}
%\\ \\
%\infer[\bot]{\Theta; \bot,\Gamma \seq \Delta}{} 
%\\ \\
% \mbox{Should add the same rules for }\Theta?
%\end{array}
%\]
%
%{\sc Asynchronous Context Rules}
%\[\begin{array}{lcl}
%  \infer[\kern-2pt\veen_r]{\jUnf{\Gamma}{\Theta}{A \veen\kern-3pt B, \Omega}{\Delta}}
%                 {\jUnf{\Gamma}{\Theta}{A,B, \Omega}{\Delta}}  
%& \qquad &
%etc
%  \end{array}
%  \]
{\sc Synchronous Context Rules}
\[\begin{array}{lcl}
\infer[{\wedgep_c}]{\jRCf{A \wedgep B}{\Gamma}}
{\jRCf{A}{\Gamma} &  \jRCf{B}{\Gamma}} 
& \qquad & etc\\ \\
\mbox{There are conditions on $\mathcal{C}$ -- see Sec.~\ref{sec:cont}}
  \end{array}
\]

{\sc Structural Context Rules}
\[\begin{array}{lcl}
\infer[\D_{C}]{\Unf{\Cx{P}}}
                    {\jRCf{P}{\Cx{P}, \Gamma}}
&\qquad &
  \infer[\krelease_{C}]{\jRCf{N}{\Gamma}}
                     {\Unf{\Cx{N}}}
\end{array}
\]
{\sc Identity Context Rules}
\[
\begin{array}{lcl}
  \infer[\kinit_{C}]{\jRCf{\ov{T}}{T,\Gamma}}{}
  \end{array}
\]
\caption{$P$ is positive, $N$ is negative, $T$ is terminal.}\label{Fig:system}
\end{figure}


\begin{example}
Assume the axiom 
\[
f^- \wedgep  \atmp{c\in A\wedge c\in B}
\] 
in the context $\Cx{\cdot} = \exists c. Point(c) \wedge^\pm l(ab) = l(ac) = l(bc)\wedge^\pm\{\cdot\}$ we would have the bipole derivation (where $\Gamma'= \Gamma,\Cx{f^- \wedgep  \atmp{c\in A\wedge c\in B}}$)
\[
\infer[\D]{\Gamma'}
{\infer[\wedgep_{C}]{\jRCf{f^- \wedgep  \atmp{c\in A\wedge c\in B}}{\Gamma'}}
{\infer[\krelease_{C}]{\jRCf{f^-}{\Gamma'}}
{\infer[\kern-2pt\nfalse]{\Cx{f^-},\Gamma'}{\Cx{\cdot},\Gamma'}}&
\infer[\kinit_{C}]{\jRCf{\atmp{c\in A\wedge c\in B}}{\Gamma'}}{}}}
\]
and the derived (unfocused) rule
\[
\infer{ \Cx{c\in A\wedge c\in B},\Gamma}
{\Cx{\cdot}\Gamma}
\]
\cyan{Observe that the $\wedgep_c$ rule can be applied since $\exists x. A\wedge B(x) \equiv A\wedge \exists x. B(x)$ where $x$ is not free in $A$.}
\end{example}

\begin{example}
\magenta{Since formulas are in negation normal form, there are no {\em negative/positive occurrences} of formulas in contexts anymore. 
However, I do not know how to generate the rule
\[
\infer{\forall x.\Cx{A(x)}\vee C(x)}{\forall x.\Cx{ B(x)}\vee C(x)}
\]
corresponding to the axiom
\[
\forall x.(A(x)\leftrightarrow B(x))\to C(x)
\]
Observe that $\forall x.(A(x)\to B(x))\to C(x)$ would be translated into 
\[
\Cx{\ov{A(x)}\vee B(x)} \mbox{ with } \Cx{\cdot}=\forall x.\{\cdot\}\vee C(x)
\]
First of all, $\forall$ does not distribute over $\vee$. Second, even if it did, focusing over a disjunction would not give the rule above.
}

\cyan{Maybe I am thinking the wrong way here. In fact, what is happening is that
\[
\forall x. (A\to B)\wedge (B\to C) \to A\to C
\]
I have to think more about that.}
\end{example}


\section{About contexts}\label{sec:cont}
Contexts should have a certain shape in order to apply the synchronous rules. 

\magenta{I am still formalizing it!}
\[
\begin{array}{lcl}
\exists x. (B1\wedge B2) & \quad\equiv \quad&  B1 \wedge  (\exists x. B2) \mbox{ if $x$ is not free in B1}\\
\forall x.(B1 \wedge  B2)  & \quad\equiv \quad&  (\forall x B1) \wedge  (\forall x. B2)\\
B1 \iimp(B2 \iimp B3)  & \equiv  & (B1 \wedge B2)\iimp B3\\
B1 \wedge (B2 \vee B3) & \equiv  & (B1 \wedge B2)\vee (B1 \wedge B3)\\
B1 \vee (B2 \wedge B3)  & \equiv  & (B1 \vee B2)\wedge (B1 \vee B3)\\
(B1 \vee B2)\iimp B3 & \equiv  & (B1 \iimp B3)\wedge (B2 \iimp B3)\\
B1 \iimp(B2 \wedge B3) &\equiv  & (B1 \iimp B2)\wedge (B1 \iimp B3)\\
B1 \iimp(\forall x. B2) & \equiv  & \forall x.(B1 \iimp B2)\\
 (\exists x.B2)\iimp B1 & \equiv &  \forall x.(B2 \iimp B1)
\end{array}
\]
\bibliography{biblio}
\end{document}

